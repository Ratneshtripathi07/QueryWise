import os
import tempfile
import hashlib
import httpx
import asyncio
import logging
from typing import List
from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import JinaEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# --- Setup ---
INDEX_DIR = "persistent_indexes"
if not os.path.exists(INDEX_DIR):
    os.makedirs(INDEX_DIR)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Initialize Clients ---
embeddings_client = JinaEmbeddings(
    jina_api_key=os.getenv("JINA_API_KEY"),
    model_name="jina-embeddings-v2-base-en"
)
llm_client = ChatGoogleGenerativeAI(model="models/gemini-2.0-flash-lite", temperature=0)

# --- Main Processing Function ---
async def process_document_and_questions(questions: List[str], doc_url: str = None, file_content: bytes = None) -> List[str]:
    try:
        if not doc_url and not file_content:
            raise ValueError("Either doc_url or file_content must be provided")
        
        if file_content:
            logger.info("Processing uploaded file content")
            # Create a hash based on file content for caching
            file_hash = hashlib.sha256(file_content).hexdigest()
            index_path = os.path.join(INDEX_DIR, f"faiss_jina_file_{file_hash}")
            
            vector_store = None
        
            if os.path.exists(index_path):
                logger.info(f"Loading existing FAISS index from: {index_path}")
                loop = asyncio.get_running_loop()
                vector_store = await loop.run_in_executor(
                    None, lambda: FAISS.load_local(index_path, embeddings_client, allow_dangerous_deserialization=True)
                )
            else:
                logger.info(f"No index found. Creating new one for uploaded file: {index_path}")
                
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(file_content)
                    tmp_file_path = tmp_file.name
            
                try:
                    loader = PyMuPDFLoader(tmp_file_path)
                    documents = loader.load()
                    
                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
                    chunks = text_splitter.split_documents(documents)
                    
                    logger.info(f"Building FAISS index for {len(chunks)} chunks using Jina API...")
                    loop = asyncio.get_running_loop()
                    vector_store = await loop.run_in_executor(
                        None, lambda: FAISS.from_documents(chunks, embeddings_client)
                    )
                    
                    await loop.run_in_executor(None, lambda: vector_store.save_local(index_path))
                    logger.info(f"Saved new index to disk: {index_path}")
                finally:
                    os.remove(tmp_file_path)
        else:
            logger.info(f"Processing request for document URL: {doc_url}")
            url_hash = hashlib.sha256(doc_url.encode()).hexdigest()
            index_path = os.path.join(INDEX_DIR, f"faiss_jina_{url_hash}")
            
            vector_store = None
            
            if os.path.exists(index_path):
                logger.info(f"Loading existing FAISS index from: {index_path}")
                loop = asyncio.get_running_loop()
                vector_store = await loop.run_in_executor(
                    None, lambda: FAISS.load_local(index_path, embeddings_client, allow_dangerous_deserialization=True)
                )
            else:
                logger.info(f"No index found. Creating new one for: {index_path}")
                async with httpx.AsyncClient(timeout=60.0) as client:
                    response = await client.get(doc_url, follow_redirects=True)
                    response.raise_for_status()

                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(response.content)
                    tmp_file_path = tmp_file.name
            
                try:
                    loader = PyMuPDFLoader(tmp_file_path)
                    documents = loader.load()
                    
                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
                    chunks = text_splitter.split_documents(documents)
                    
                    logger.info(f"Building FAISS index for {len(chunks)} chunks using Jina API...")
                    loop = asyncio.get_running_loop()
                    vector_store = await loop.run_in_executor(
                        None, lambda: FAISS.from_documents(chunks, embeddings_client)
                    )
                    
                    await loop.run_in_executor(None, lambda: vector_store.save_local(index_path))
                    logger.info(f"Saved new index to disk: {index_path}")
                finally:
                    os.remove(tmp_file_path)

        # --- RAG Setup ---
        retriever = vector_store.as_retriever(search_kwargs={"k": 7})
        prompt_template = """
        You are an expert AI data extractor. Answer with precision based ONLY on the provided context.
        INSTRUCTIONS:
        - Use bullet points for multiple parts.
        - For a single value, provide only that value.
        - Be direct. No introductory phrases.
        CONTEXT:
        {context}
        QUESTION:
        {question}
        PRECISE ANSWER:
        """
        prompt = PromptTemplate.from_template(prompt_template)
        rag_chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt | llm_client | StrOutputParser())

        # --- Process Questions Concurrently ---
        async def process_single_question(question: str) -> str:
            try:
                return (await rag_chain.ainvoke(question)).strip()
            except Exception as e:
                logger.error(f"Error for question '{question[:50]}...': {e}")
                return "Error processing this question."

        tasks = [process_single_question(q) for q in questions]
        return await asyncio.gather(*tasks)
    except Exception as e:
        logger.error(f"A critical error occurred: {e}", exc_info=True)
        return [f"A critical error occurred: {e}"] * len(questions)